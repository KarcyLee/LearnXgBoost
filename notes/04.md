> 这节主要分析GradientBoost 和 predictor

# 1. 接口 GradientBooster

在gbm.h中定义。 Class GradientBoost ,是一个抽象接口，它定义了 Gradient Boost 的抽象接口。其派生出的两个类 Class GBTree 和 Class GBLinear 分别对应着配置文件里面的参数"gbtree"和"gblinear"

- Class GBTree 主要使用的回归树作为其弱分类器
- Class GBLinear 使用的是线性回归或逻辑回归作为其弱分类器

重点关注DoBoost() 和 PredictBatch()函数

```c++
class GradientBooster {
 protected:
  GenericParameter const* learner_param_;

 public:
  /*! \brief virtual destructor */
  virtual ~GradientBooster() = default;
  /*!
   * \brief Set the configuration of gradient boosting.
   *  User must call configure once before InitModel and Training.
   *
   * \param cfg configurations on both training and model parameters.
   */
  virtual void Configure(const std::vector<std::pair<std::string, std::string> >& cfg) = 0;

  virtual void Load(dmlc::Stream* fi) = 0;
  virtual void Save(dmlc::Stream* fo) const = 0;
  virtual bool AllowLazyCheckPoint() const {
    return false;
  }
  /*!
   * \brief perform update to the model(boosting)
   * \param p_fmat feature matrix that provide access to features
   * \param in_gpair address of the gradient pair statistics of the data
   * \param obj The objective function, optional, can be nullptr when use customized version
   * the booster may change content of gpair
   */
  virtual void DoBoost(DMatrix* p_fmat,
                       HostDeviceVector<GradientPair>* in_gpair,
                       ObjFunction* obj = nullptr) = 0;

  /*!
   * \brief generate predictions for given feature matrix
   * \param dmat feature matrix
   * \param out_preds output vector to hold the predictions
   * \param ntree_limit limit the number of trees used in prediction, when it equals 0, this means
   *    we do not limit number of trees, this parameter is only valid for gbtree, but not for gblinear
   */
  virtual void PredictBatch(DMatrix* dmat,
                            HostDeviceVector<bst_float>* out_preds,
                            unsigned ntree_limit = 0) = 0;
  /*!
   * \brief online prediction function, predict score for one instance at a time
   *  NOTE: use the batch prediction interface if possible, batch prediction is usually
   *        more efficient than online prediction
   *        This function is NOT threadsafe, make sure you only call from one thread
   *
   * \param inst the instance you want to predict
   * \param out_preds output vector to hold the predictions
   * \param ntree_limit limit the number of trees used in prediction
   * \param root_index the root index
   * \sa Predict
   */
  virtual void PredictInstance(const SparsePage::Inst& inst,
                       std::vector<bst_float>* out_preds,
                       unsigned ntree_limit = 0,
                       unsigned root_index = 0) = 0;
  /*!
   * \brief predict the leaf index of each tree, the output will be nsample * ntree vector
   *        this is only valid in gbtree predictor
   * \param dmat feature matrix
   * \param out_preds output vector to hold the predictions
   * \param ntree_limit limit the number of trees used in prediction, when it equals 0, this means
   *    we do not limit number of trees, this parameter is only valid for gbtree, but not for gblinear
   */
  virtual void PredictLeaf(DMatrix* dmat,
                           std::vector<bst_float>* out_preds,
                           unsigned ntree_limit = 0) = 0;

  /*!
   * \brief feature contributions to individual predictions; the output will be a vector
   *         of length (nfeats + 1) * num_output_group * nsample, arranged in that order
   * \param dmat feature matrix
   * \param out_contribs output vector to hold the contributions
   * \param ntree_limit limit the number of trees used in prediction, when it equals 0, this means
   *    we do not limit number of trees
   * \param approximate use a faster (inconsistent) approximation of SHAP values
   * \param condition condition on the condition_feature (0=no, -1=cond off, 1=cond on).
   * \param condition_feature feature to condition on (i.e. fix) during calculations
   */
  virtual void PredictContribution(DMatrix* dmat,
                           std::vector<bst_float>* out_contribs,
                           unsigned ntree_limit = 0, bool approximate = false,
                           int condition = 0, unsigned condition_feature = 0) = 0;

  virtual void PredictInteractionContributions(DMatrix* dmat,
                           std::vector<bst_float>* out_contribs,
                           unsigned ntree_limit, bool approximate) = 0;

  virtual std::vector<std::string> DumpModel(const FeatureMap& fmap,
                                             bool with_stats,
                                             std::string format) const = 0;
  virtual bool UseGPU() const = 0;
  /*!
   * \brief create a gradient booster from given name
   * \param name name of gradient booster
   * \param cache_mats The cache data matrix of the Booster.
   * \param base_margin The base margin of prediction.
   * \return The created booster.
   */
  static GradientBooster* Create(
      const std::string& name,
      GenericParameter const* gparam,
      const std::vector<std::shared_ptr<DMatrix> >& cache_mats,
      bst_float base_margin);
};
```

# 2. 接口Predictor 

Predictor 预测器对象，主要在GradientBooster类中调用，用来预测输出，后续根据预测值才能计算一阶梯度与二阶梯度：

- CPUPredictor 默认
- GPUPredictor

主要关注PredictBatch()函数。

其中CPUPredictor的PredictBatch()基本流程为：

```
|-- CPUPredictor::PredictBatch()
		// 1. 查看上次训练完是否对eval数据集进行评估，train数据集初始化时早已合并到eval数据集，如果找到就直接使用评估后的预测值，返回true。第一次调用时，由于还未训练，直接返回false，进行下面步骤。
		|-- PredictFromCache()
		
		// 2. 将预测值设置为base_margin。如果用户在训练数据文件中对实例设置base_margin，则使用设置值，否则使用参数设置model.base_margin。 
		|-- InitOutPredictions()
		
		// 3. 以base_margin为初始值，使用GBTreeModel对象进行预测。实际上该对象封装了训练到当前迭代的所有回归树集合，额外判断是否为多分类任务，即num_output_group!=1。使用不同参数，调用预测核心方法PredLoopSpecalize()。该函数中通过K=8设置预测的local batch，利用OMP多线程对记录并行预测。每个线程内按照local batch执行。为什么使用local batch，具备缓存预取功能。其他地方也有使用这种技术。
		|-- PredLoopInternal()
```

# 3. 目标函数/损失函数ObjFunction 接口



# 4. 度量函数Metric接口



