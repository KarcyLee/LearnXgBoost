> 本节主要讲xgboost原理
>
> [xgboost-doc](https://xgboost.readthedocs.io/en/latest/tutorials/model.html)
>
> [XGBoost: A Scalable Tree Boosting System](https://www.kdd.org/kdd2016/papers/files/rfp0697-chenAemb.pdf)
>
> [论文精读](https://blog.csdn.net/Dby_freedom/article/details/84301725)

[TOC]

# 1. Introduction

四个创新点

- 设计和构建高度可扩展的端到端boosting tree系统 
- 提出了一个理论上合理的加权分位数略图(a theoretically justified weighted quantile sketch for efficient proposal calculation) 。 主要用来寻找“推荐分割点”，不用遍历所有的点，只用部分点就行，近似地表示，省时间。
- 引入了一种新颖的稀疏感知算法用于并行树学习, 令缺失值有默认方向。
- 提出了一个有效的用于核外树形学习的缓存感知块结构（cache-aware block structure）。 用缓存加速寻找排序后被打乱的索引的列数据的过程

# 2. Tree Boosting

dataset with $n$ examples and $m$ features:

$\mathcal{D}=\left\{\left(\mathbf{x}_{i}, y_{i}\right)\right\}\left(|\mathcal{D}|=n, \mathbf{x}_{i} \in \mathbb{R}^{m}, y_{i} \in \mathbb{R}\right)$

由K个additive functions 构成的tree ensemble model:

$\hat{y}_{i}=\phi\left(\mathbf{x}_{i}\right)=\sum_{k=1}^{K} f_{k}\left(\mathbf{x}_{i}\right), \quad f_{k} \in \mathcal{F}$

其中，$\mathcal{F}=\left\{f(\mathbf{x})=w_{q(\mathbf{x})}\right\}\left(q : \mathbb{R}^{m} \rightarrow T, w \in \mathbb{R}^{T}\right)$ 为回归树空间（CART）

$q$代表树结构，$T$和$w$为树的参数；$T$代表叶子个数，$w$表示叶子权重。

$q(x)$： 输入为x时，这棵树的输出。（特别地，xgboost中，$q$将$x$映射到特定的叶节点上）。

回归树：

和decision tree 不同，每个regression tree的每个leaf都有一个连续权重$w_i$。 回归的流程如下：

for a given example

1. 按照decision rule对其在每一棵树上分类，（每棵树最后走到一个叶子节点）
2. 对这些叶子节点进行加权求和，即为回归值

## 2.1 目标函数 （regularized objective）

目标函数由模型经验损失+正则项构成：
$$
\begin{array}{l}{\mathcal{L}(\phi)=\sum_{i} l\left(\hat{y}_{i}, y_{i}\right)+\sum_{k} \Omega\left(f_{k}\right)} \\ {\text { where } \Omega(f)=\gamma T+\frac{1}{2} \lambda\|w\|^{2}}\end{array}
$$
其中:

$T$为叶子个数，$w$为叶子权重,每个$f_k$对应于一个独立的树结构$q$和叶子权重$w$。$l$ 为一个可微凸loss函数(differentiable convex loss function),  可以计算预测值$\hat{y}_{i}$ 与$y_i$直接的loss.

$\Omega$为惩罚项，惩罚模型复杂度。正则项可以对最终学到的权重进行平滑，避免overfitting。相类似的正则化技术也用在RGF模型(Regu- larized greedy forest)上。XGBoost的目标函数与相应的学习算法比RGF简单，更容易并行化。当正则参数设置为0时，目标函数退化为传统的gradient tree boosting。

## 2.2 Gradient Tree Boosting

上述目标函数不能使用在欧拉空间中的传统优化方法进行优化， instead, 模型以一种叠加的方式进行训练。

定义$\hat{y_i}^{(t)} $为第i个实例在第t次迭代（应该是第t棵树？）时的预测，我们需要添加$f_t$来最小化如下目标函数：
$$
\mathcal{L}^{(t)}=\sum_{i=1}^{n} l\left(y_{i}, \hat{y}_{i}^{(t-1)}+f_{t}\left(\mathbf{x}_{i}\right)\right)+\Omega\left(f_{t}\right)
$$
其中，这意味着，我们greedily添加每一棵树 $f_t$，最终尽可能的提升ensemble模型。 使用二阶近似可以快速优化目标函数。

> 二阶泰勒展开
>
> $f(x+\Delta x) \simeq f(x)+f^{\prime}(x) \Delta x+\frac{1}{2} f^{\prime \prime}(x) \Delta x^{2}$

定义 每个数据点的在loss function上的一阶导数、二阶导数分别为：
$$
g_{i}=\partial_{\hat{y}(t-1)} l\left(y_{i}, \hat{y}^{(t-1)}\right), h_{i}=\partial_{\hat{y}(t-1)}^{2} l\left(y_{i}, \hat{y}^{(t-1)}\right)
$$
二阶泰勒展开形式为：
$$
\mathcal{L}^{(t)} \simeq \sum_{i=1}^{n}\left[l\left(y_{i}, \hat{y}^{(t-1)}\right)+g_{i} f_{t}\left(\mathbf{x}_{i}\right)+\frac{1}{2} h_{i} f_{t}^{2}\left(\mathbf{x}_{i}\right)\right]+\Omega\left(f_{t}\right)
$$


移除常数项，从而获得如下所示的在$t$次迭代时的简化版目标函数：
$$
\mathcal{L}^{(t)}=\sum_{i=1}^{n}\left[g_{i} f_{t}\left(x_{i}\right)+\frac{1}{2} h_{i} f_{t}^{2}\left(x_{i}\right)\right]+\Omega\left(f_{t}\right)
$$
进一步，对 XGboost 来说，每一个数据点$x_i$ 最终会落到一个叶子结点上，而对于落在同一个叶子结点上的数据点来说，其输出都是一样的，假设我们共有$J$ 个叶子结点，每个叶子结点对应的输出为$w_j$ ($w_j$ 也是我们想要求解最优权重)。定义每个叶子节点$j$上的样本集合$I_{j}=\left\{i | q\left(x_{i}\right)=j\right\}$，扩展$\Omega$将上述loss function重写为：
$$
\begin{aligned} \tilde{\mathcal{L}}^{(t)} &=\sum_{i=1}^{n}\left[g_{i} f_{t}\left(\mathbf{x}_{i}\right)+\frac{1}{2} h_{i} f_{t}^{2}\left(\mathbf{x}_{i}\right)\right]+\gamma T+\frac{1}{2} \lambda \sum_{j=1}^{T} w_{j}^{2} \\ &=\sum_{j=1}^{T}\left[\left(\sum_{i \in I_{j}} g_{i}\right) w_{j}+\frac{1}{2}\left(\sum_{i \in I_{j}} h_{i}+\lambda\right) w_{j}^{2}\right]+\gamma T \end{aligned}
$$

> 针对每棵树，由遍历样本  -> 遍历每个叶子节点



因此，现在要做的是两件事：

- 确定树的结构$q(x)$.
- 针对$q(x)$求最优的叶节点权重，使得loss最小

### 2.2.1 针对固定树结构，求最优$w$

针对固定的树$q(x)$， 可以计算每个叶节点的最优权重$w_{j}^{*}$:
$$
w_{j}^{*}=-\frac{\sum_{i \in I_{j}} g_{i}}{\sum_{i \in I_{j}} h_{i}+\lambda}
$$
将权重带回我们的目标函数，目标函数变为:
$$
\tilde{\mathcal{L}}^{(t)}(q)=-\frac{1}{2} \sum_{j=1}^{T} \frac{\left(\sum_{i \in I_{j}} g_{i}\right)^{2}}{\sum_{i \in I_{j}} h_{i}+\lambda}+\gamma T
$$
这个目标函数可以作为一个得分函数（scoring function）来衡量一棵树结构q的质量（quality），也可以被称作structure score。

> 可以看出，为使得loss function最小，则$\frac{\left(\sum_{i \in I_{i}} g_{i}\right)^{2}}{\sum_{i \in I_{j}} h_{i}+\lambda}$越大越好。

### 2.2.2 确定树结构

暴力枚举所有的树结构，然后选择structure score最小的。 树的结构太多了，这样枚举一般不可行。

通常采用贪心法，每次尝试分裂一个叶节点，计算分裂后的增益，选增益最大的。

xgboost的增益计算法方法为：

*Gain = 1/2 [左子树分数 + 右子树分数 - 分裂前分数] - 新叶节点复杂度*。

则可以得到一个简单的决定是否可以进行分裂的判断标准，即**分裂之后两边的目标函数之和是否能够大于不分裂的目标函数值**。

​        通常，**不可能枚举所有可能的树结构q**。而贪婪算法会从单个叶子出发，迭代添加分枝到树中。假设$I_L$ 和 $I_R$是一次划分(split)后的左节点和右节点所对应的实例集合；$$I=I_{L} \cup I_{R}$$；在split之后的loss reduction为：
$$
\mathcal{L}_{\text {split}}=\frac{1}{2}\left[\frac{\left(\sum_{i \in I_{L}} g_{i}\right)^{2}}{\sum_{i \in I_{L}} h_{i}+\lambda}+\frac{\left(\sum_{i \in I_{R}} g_{i}\right)^{2}}{\sum_{i \in I_{R}} h_{i}+\lambda}-\frac{\left(\sum_{i \in I} g_{i}\right)^{2}}{\sum_{i \in I} h_{i}+\lambda}\right]-\gamma
$$
