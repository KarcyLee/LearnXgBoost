> 本节主要讲xgboost原理
>
> [xgboost-doc](https://xgboost.readthedocs.io/en/latest/tutorials/model.html)
>
> [XGBoost: A Scalable Tree Boosting System](https://www.kdd.org/kdd2016/papers/files/rfp0697-chenAemb.pdf)
>
> [论文精读](https://blog.csdn.net/Dby_freedom/article/details/84301725)

[TOC]

# 1. Introduction

四个创新点

- 设计和构建高度可扩展的端到端boosting tree系统 
- 提出了一个理论上合理的加权分位数略图(a theoretically justified weighted quantile sketch for efficient proposal calculation) 。 主要用来寻找“推荐分割点”，不用遍历所有的点，只用部分点就行，近似地表示，省时间。
- 引入了一种新颖的稀疏感知算法用于并行树学习, 令缺失值有默认方向。
- 提出了一个有效的用于核外树形学习的缓存感知块结构（cache-aware block structure）。 用缓存加速寻找排序后被打乱的索引的列数据的过程

# 2. Tree Boosting

dataset with $n$ examples and $m$ features:

$\mathcal{D}=\left\{\left(\mathbf{x}_{i}, y_{i}\right)\right\}\left(|\mathcal{D}|=n, \mathbf{x}_{i} \in \mathbb{R}^{m}, y_{i} \in \mathbb{R}\right)$

由K个additive functions 构成的tree ensemble model:

$\hat{y}_{i}=\phi\left(\mathbf{x}_{i}\right)=\sum_{k=1}^{K} f_{k}\left(\mathbf{x}_{i}\right), \quad f_{k} \in \mathcal{F}$

其中，$\mathcal{F}=\left\{f(\mathbf{x})=w_{q(\mathbf{x})}\right\}\left(q : \mathbb{R}^{m} \rightarrow T, w \in \mathbb{R}^{T}\right)$ 为回归树空间（CART）

$q$代表树结构，$T$代表叶子个数，$w$表示叶子权重。

回归树：

和decision tree 不同，每个regression tree的每个leaf都有一个连续权重$w_i$。 回归的流程如下：

for a given example

1. 按照decision rule对其在每一棵树上分类，（每棵树最后走到一个叶子节点）
2. 对这些叶子节点进行加权求和，即为回归值

## 2.1 目标函数 （regularized objective）

目标函数由模型经验损失+正则项构成：

$$\begin{array}{l}{\mathcal{L}(\phi)=\sum_{i} l\left(\hat{y}_{i}, y_{i}\right)+\sum_{k} \Omega\left(f_{k}\right)} \\ {\text { where } \Omega(f)=\gamma T+\frac{1}{2} \lambda\|w\|^{2}}\end{array}$$

其中，$T$为叶子个数，$w$为叶子权重。

改写后为：

$$\mathcal{L}^{(t)}=\sum_{i=1}^{n} l\left(y_{i}, \hat{y}_{i}^{(t-1)}+f_{t}\left(\mathbf{x}_{i}\right)\right)+\Omega\left(f_{t}\right)$$

其中，





